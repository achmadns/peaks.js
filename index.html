<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8" content="text/html">
  <meta name="viewport" content="width=device-width">
  <title>Peaks.js Demo Page</title>
  <style>

    #titles {
      font-family: 'Helvetica neue', Helvetica, Arial, sans-serif;
    }

    #titles, [id*="waveform-visualiser"] {
      margin: 24px auto;
      width: 1000px;
    }

    [id*="waveform-visualiser"] [class*="-container"] {
      box-shadow: 3px 3px 20px #919191;
      margin: 0 0 24px 0;
      -moz-box-shadow: 3px 3px 20px #919191;
      -webkit-box-shadow: 3px 3px 20px #919191;
      line-height: 0;
    }

    .overview-container {
      height: 85px;
    }

    #second-waveform-visualiser-container [class*="-container"] {
      background: #111;
    }

    #demo-controls {
      margin: 0 auto 24px auto;
      width: 1000px;
    }

    #demo-controls > * {
      vertical-align: middle;
    }

    #demo-controls button {
      background: #fff;
      border: 1px solid #919191;
      cursor: pointer;
    }
  </style>
</head>
<body>
<div id="titles">
  <h1>Peaks.js Demo</h1>

  <p>Peaks.js is a modular client-side JavaScript component designed for
    the display of and interaction with audio waveforms in the browser.</p>

  <p>Peaks.js was developed by <a href="http://www.bbc.co.uk/rd">BBC R&amp;D</a>
    to allow users to make accurate clippings of audio content in the browser.</p>

  <p>Peaks.js uses HTML5 canvas technology to display the audio waveform at
    different zoom levels and provides some basic convenience methods for
    interacting with waveforms and creating time-based visual sections for
    denoting content to be clipped or for reference eg: distinguishing music
    from speech or identifying different music tracks.</p>

  <p>You can read more about the project
    <a href="http://waveform.prototyping.bbc.co.uk/">here</a>.</p>
</div>

<div id="first-waveform-visualiser-container"></div>

<div id="demo-controls">
  <audio controls=controls id="audio" crossorigin="anonymous">
    <!--<source src="/test_data/TOL_6min_720p_download.mp3" type="audio/mpeg">-->
    <source src="http://icecast.local:8000/audio.ogg" type="audio/ogg">
    <source src="test_data/sample.ogg" type="audio/ogg">
    Your browser does not support the audio element.
  </audio>

  <button data-action="zoom-in">Zoom in</button>
  <button data-action="zoom-out">Zoom out</button>
  <button data-action="add-segment">Add a Segment at current time</button>
  <button data-action="add-point">Add a Point at current time</button>
  <button data-action="log-data">Log segments/points</button>
</div>

<script src="node_modules/waveform-data/dist/waveform-data.js"></script>
<script src="node_modules/waveform-data/dist/webaudio.js"></script>
<script src="peaks.js"></script>
<script>
  (function (Peaks) {
    const WaveformData = window.WaveformData;
    const webAudioBuilder = window.WaveformDataWebaudioBuilder;
    const audioContext = new AudioContext();
    const audioElement = document.getElementById('audio');
    const backupAudioSource = audioContext.createMediaElementSource(audioElement);
    const audioSource = audioContext.createMediaElementSource(audioElement);
    const analyser = audioContext.createAnalyser();
    const scriptProcessor = audioContext.createScriptProcessor(16384, 1, 1);
    backupAudioSource.connect(audioContext.destination);
    // Bind our analyser to the media element source.
//    audioSrc.connect(analyser);
//    audioSrc.connect(audioContext.destination);


    const processedWaveform = function (error, waveform) {
      if (error)
        console.error('Ooops! ' + error.message);
      else
        console.log(waveform.duration);
    };

    scriptProcessor.onaudioprocess = function (audioProcessEvent) {
      // inputBuffer is AudioBuffer
      const inputBuffer = audioProcessEvent.inputBuffer;
      // pcm is ArrayBuffer
      const pcm = inputBuffer.getChannelData(0).buffer;
      var temporaryBuffer = new Float32Array(inputBuffer.length);
      inputBuffer.copyFromChannel(temporaryBuffer, 0, 0);
      // do this so that audio still plays
      audioProcessEvent.outputBuffer.copyToChannel(temporaryBuffer, 0, 0);
//      const output = audioProcessEvent.outputBuffer.getChannelData(0);
//      buffer.map(function (i, value) {
//        output[i] = value;
//      });
//      audioProcessEvent.outputBuffer = audioProcessEvent.inputBuffer;
//      webAudioBuilder(audioContext, new Blob([buffer], {type: 'audio/ogg'}), processedWaveform);
//      const waveForm = WaveformData.create(new Blob([buffer], {type: 'audio/ogg'}));
//      console.log('waveform: ' + waveForm);

    };

    audioSource.connect(scriptProcessor);
//    scriptProcessor.connect(audioContext.destination);

//    audioSrc.connect(audioContext.destination);

    var requestAnimationFrameId;

    function getData() {
      requestAnimationFrameId = requestAnimationFrame(getData);
      var dataArray = new Float32Array(analyser.frequencyBinCount);
      analyser.getFloatFrequencyData(dataArray);
      var buffer = new ArrayBuffer(dataArray.length);
      dataArray.map(function (i, value) {
        buffer[i] = value;
      });
      webAudioBuilder(audioContext, buffer, processedWaveform);
//      var waveForm = WaveformData.create(buffer);
      console.log('data array: ' + dataArray);
    };
    audioElement.onplay = function () {
//      getData();
    };
    function stopDataRequest() {
      cancelAnimationFrame(requestAnimationFrameId);
    };
    audioElement.onended = function () {
//      stopDataRequest();
    };
    audioElement.onpause = function () {
//      stopDataRequest();
    };
    var options = {
      container: document.getElementById('first-waveform-visualiser-container'),
      mediaElement: document.querySelector('audio'),
      audioContext: audioContext,
      keyboard: false
    };

    var peaksInstance = Peaks.init(options);

    document.querySelector('[data-action="zoom-in"]').addEventListener("click", peaksInstance.zoom.zoomIn.bind(peaksInstance));
    document.querySelector('[data-action="zoom-out"]').addEventListener("click", peaksInstance.zoom.zoomOut.bind(peaksInstance));

    document.querySelector('button[data-action="add-segment"]').addEventListener("click", function () {
      peaksInstance.segments.add({
        startTime: peaksInstance.time.getCurrentTime(),
        endTime: peaksInstance.time.getCurrentTime() + 10,
        editable: true
      });
    });

    document.querySelector('button[data-action="add-point"]').addEventListener("click", function () {
      peaksInstance.points.add({
        timestamp: peaksInstance.time.getCurrentTime(),
        editable: true
      });
    });

    document.querySelector('button[data-action="log-data"]').addEventListener("click", function (event) {
      console.log('Segments', peaksInstance.segments.getSegments());
      console.log('Points', peaksInstance.points.getPoints());
    });
  })(peaks);
</script>
</body>
</html>
