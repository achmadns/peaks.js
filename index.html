<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8" content="text/html">
  <meta name="viewport" content="width=device-width">
  <title>Peaks.js Demo Page</title>
  <style>

    #titles {
      font-family: 'Helvetica neue', Helvetica, Arial, sans-serif;
    }

    #titles, [id*="waveform-visualiser"] {
      margin: 24px auto;
      width: 1000px;
    }

    [id*="waveform-visualiser"] [class*="-container"] {
      box-shadow: 3px 3px 20px #919191;
      margin: 0 0 24px 0;
      -moz-box-shadow: 3px 3px 20px #919191;
      -webkit-box-shadow: 3px 3px 20px #919191;
      line-height: 0;
    }

    .overview-container {
      height: 85px;
    }

    #second-waveform-visualiser-container [class*="-container"] {
      background: #111;
    }

    #demo-controls {
      margin: 0 auto 24px auto;
      width: 1000px;
    }

    #demo-controls > * {
      vertical-align: middle;
    }

    #demo-controls button {
      background: #fff;
      border: 1px solid #919191;
      cursor: pointer;
    }
  </style>
</head>
<body>
<div id="titles">
  <h1>Peaks.js Demo</h1>

  <p>Peaks.js is a modular client-side JavaScript component designed for
    the display of and interaction with audio waveforms in the browser.</p>

  <p>Peaks.js was developed by <a href="http://www.bbc.co.uk/rd">BBC R&amp;D</a>
    to allow users to make accurate clippings of audio content in the browser.</p>

  <p>Peaks.js uses HTML5 canvas technology to display the audio waveform at
    different zoom levels and provides some basic convenience methods for
    interacting with waveforms and creating time-based visual sections for
    denoting content to be clipped or for reference eg: distinguishing music
    from speech or identifying different music tracks.</p>

  <p>You can read more about the project
    <a href="http://waveform.prototyping.bbc.co.uk/">here</a>.</p>
</div>

<div id="first-waveform-visualiser-container"></div>

<div id="demo-controls">
  <audio controls=controls id="audio" crossorigin="anonymous">
    <!--<source src="http://icecast.local:8000/audio.ogg" type="audio/ogg">-->
    <!--https://developer.mozilla.org/en-US/docs/Web/HTTP/Basics_of_HTTP/MIME_types-->
    <!--<source src="/test_data/TOL_6min_720p_download.mp3" type="audio/mpeg">-->
    <!--<source src="test_data/button.mp3" type="audio/mpeg">-->
    <!--<source src="test_data/bells.mp3" type="audio/mpeg">-->
    <source src="audio/faatihah-13-secs.mp3" type="audio/mpeg">
    Your browser does not support the audio element.
  </audio>

  <button data-action="zoom-in">Zoom in</button>
  <button data-action="zoom-out">Zoom out</button>
  <button data-action="add-segment">Add a Segment at current time</button>
  <button data-action="add-point">Add a Point at current time</button>
  <button data-action="log-data">Log segments/points</button>
</div>

<script src="node_modules/waveform-data/dist/waveform-data.js"></script>
<script src="node_modules/waveform-data/dist/webaudio.js"></script>
<script src="node_modules/webaudio-peaks/dist/webaudio-peaks.umd.js"></script>
<script src="peaks.js"></script>
<script>
  (function (Peaks) {
    const WaveformData = window.WaveformData;
    const webAudioBuilder = window.WaveformDataWebaudioBuilder;
    const audioContext = new AudioContext();
    const audioElement = document.getElementById('audio');
    const audioSource = audioContext.createMediaElementSource(audioElement);
    const analyserSource = audioContext.createMediaElementSource(audioElement);
    const scriptSource = audioContext.createMediaElementSource(audioElement);
    const analyser = audioContext.createAnalyser();
    const scriptProcessor = audioContext.createScriptProcessor();
    // connect the audio to speaker
    audioSource.connect(audioContext.destination);

    const processedWaveform = function (error, waveform) {
      if (error)
        console.error('Ooops! ' + error.message);
      else
        console.log(waveform.duration);
    };
    var connected = false;

    scriptProcessor.onaudioprocess = function (audioProcessEvent) {
      // inputBuffer is AudioBuffer
      const inputBuffer = audioProcessEvent.inputBuffer;
      // pcm is ArrayBuffer
      const channelData = inputBuffer.getChannelData(0);
//      const pcm = channelData.buffer;
//      console.log('sample rate;length;channels;buffer byte length;duration are ' + inputBuffer.sampleRate + ';' + inputBuffer.length + ';' + inputBuffer.numberOfChannels + ';' + pcm.byteLength + ';' + inputBuffer.duration);
//      console.log('pcm:' + new Float32Array(pcm));
//      console.log("peaks: " + extractPeaks(channelData, 512, 8));
      const peaks = webaudioPeaks(channelData, 512, 8);
      console.log("peaks: " + peaks.length + ";" + peaks.data[0]);
//      var temporaryBuffer = new Float32Array(inputBuffer.length);
//      inputBuffer.copyFromChannel(temporaryBuffer, 0, 0);
      // do this so that audio still plays
//      audioProcessEvent.outputBuffer.copyToChannel(temporaryBuffer, 0, 0);
//      const output = audioProcessEvent.outputBuffer.getChannelData(0);
//      buffer.map(function (i, value) {
//        output[i] = value;
//      });
//      audioProcessEvent.outputBuffer = audioProcessEvent.inputBuffer;
//      webAudioBuilder(audioContext, new Blob([buffer], {type: 'audio/ogg'}), processedWaveform);
//      const waveForm = WaveformData.create(new Blob([buffer], {type: 'audio/ogg'}));
//      console.log('waveform: ' + waveForm);

    };

    function connectAnalysers() {
      if (connected) return;
      // bind our analyser to the media element source.
//      analyserSource.connect(analyser);
      scriptSource.connect(scriptProcessor);
      console.log('Audio ready to be analysed.');
      connected = true;
    }

    var requestAnimationFrameId;

    function getData() {
      requestAnimationFrameId = requestAnimationFrame(getData);
      var dataArray = new Float32Array(analyser.frequencyBinCount);
      analyser.getFloatFrequencyData(dataArray);
      var buffer = new ArrayBuffer(dataArray.length);
      dataArray.map(function (i, value) {
        buffer[i] = value;
      });
//      webAudioBuilder(audioContext, buffer, processedWaveform);
//      var waveForm = WaveformData.create(buffer);
      console.log('data array: ' + dataArray);
    };

    function disconnectAnalysers() {
      scriptSource.disconnect();
//      analyserSource.disconnect();
    };

    audioElement.onplay = function () {
//      getData();
      connectAnalysers();
    };
    function stopDataRequest() {
//      cancelAnimationFrame(requestAnimationFrameId);
      disconnectAnalysers();
    };

    audioElement.onended = function () {
//      stopDataRequest();
      disconnectAnalysers();
    };
    audioElement.onpause = function () {
//      stopDataRequest();
//      disconnectAnalysers();
    };
    var options = {
      container: document.getElementById('first-waveform-visualiser-container'),
      mediaElement: document.querySelector('audio'),
      dataUri: {
        /*arraybuffer: '/audio/faatihah-13-secs-512p.dat',*/
        json: 'peaks.json'
      },
//      audioContext: audioContext,
      keyboard: false,
      zoomLevels: [512, 1024, 2048, 4096]
    };

    var peaksInstance = Peaks.init(options);
    peaksInstance.zoom.setZoom(0);


    document.querySelector('[data-action="zoom-in"]').addEventListener("click", peaksInstance.zoom.zoomIn.bind(peaksInstance));
    document.querySelector('[data-action="zoom-out"]').addEventListener("click", peaksInstance.zoom.zoomOut.bind(peaksInstance));

    document.querySelector('button[data-action="add-segment"]').addEventListener("click", function () {
      peaksInstance.segments.add({
        startTime: peaksInstance.time.getCurrentTime(),
        endTime: peaksInstance.time.getCurrentTime() + 10,
        editable: true
      });
    });

    document.querySelector('button[data-action="add-point"]').addEventListener("click", function () {
      peaksInstance.points.add({
        timestamp: peaksInstance.time.getCurrentTime(),
        editable: true
      });
    });

    document.querySelector('button[data-action="log-data"]').addEventListener("click", function (event) {
      console.log('Segments', peaksInstance.segments.getSegments());
      console.log('Points', peaksInstance.points.getPoints());
    });
  })(peaks);
</script>
</body>
</html>
